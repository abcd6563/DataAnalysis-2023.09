{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰화(Tokenization)\n",
    "- 딥 러닝을 이용한 자연어 처리 입문\n",
    "- 2장. 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk(Natural Language Toolkit) 필요 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\human\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\human\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\human\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "방법 1: ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n",
    "print('방법 1:', word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "방법2 : ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print('방법2 :', WordPunctTokenizer().tokenize(text)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화1 : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "print('문장 토큰화1 :', sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "print('문장 토큰화2 :', sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한글 문장 토큰화\n",
    "    - conda activate base\n",
    "    - pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\WorkSpace\\02.DataAnalysis\\06.텍스트분석\\01.토큰화.ipynb 셀 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/WorkSpace/02.DataAnalysis/06.%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%B6%84%EC%84%9D/01.%ED%86%A0%ED%81%B0%ED%99%94.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkss\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/WorkSpace/02.DataAnalysis/06.%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%B6%84%EC%84%9D/01.%ED%86%A0%ED%81%B0%ED%99%94.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/WorkSpace/02.DataAnalysis/06.%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%B6%84%EC%84%9D/01.%ED%86%A0%ED%81%B0%ED%99%94.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m한국어 문장 토큰화 :\u001b[39m\u001b[39m'\u001b[39m, kss\u001b[39m.\u001b[39msplit_sentences(text))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kss'"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n",
    "print('한국어 문장 토큰화 :', kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 한글 토큰화\n",
    "- Java JDK 설치\n",
    "- pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['딥', '러닝', '자연어', '처리', '문제', '영어', '한국어', '때', '이제', '알걸']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "okt.nouns(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 : ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
      "품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "\n",
    "print('단어 토큰화 :', tokenized_sentence)\n",
    "print('품사 태깅 :', pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
    "# 형태소 분석\n",
    "okt.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가보다']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.morphs(text, stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('열심히', 'Adverb'),\n",
       " ('코딩', 'Noun'),\n",
       " ('한', 'Josa'),\n",
       " ('당신', 'Noun'),\n",
       " (',', 'Punctuation'),\n",
       " ('연휴', 'Noun'),\n",
       " ('에는', 'Josa'),\n",
       " ('여행', 'Noun'),\n",
       " ('을', 'Josa'),\n",
       " ('가봐요', 'Verb')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 품사 태깅\n",
    "okt.pos(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['코딩', '당신', '연휴', '여행']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 명사 추출\n",
    "okt.nouns(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
